---
title: å›½åº†çˆ¬ä¸ªæ˜æ—¥æ–¹èˆŸç«‹ç»˜å¹¶æ­å»ºapi
published: 2024-10-05
description: è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä½œè€…ä¸ºå¤§å®¶å‡†å¤‡äº†ä¸€ä»½å›½åº†ç¤¼ç‰©â€”â€”ä¸€ä¸ªçˆ¬è™«ç¨‹åºï¼Œå¹¶æ„Ÿè°¢äº†ä¸€äº›å¤§ä½¬ã€‚ä½œè€…åœ¨æ–°å¹´å‰è¦çˆ¬å–ä¸€äº›æ•°æ®ï¼Œå¹¶å¸Œæœ›èƒ½å’Œå¤§å®¶ä¸€èµ·è¿›æ­¥ã€‚
tags: [çˆ¬è™«, Worker]
category: æŠ€æœ¯æ•™ç¨‹
draft: false
---

å¤§å®¶å›½åº†å¿«ä¹å‘€ï¼Œå†™äº†ä¸ªçˆ¬è™«é€ç»™å¤§å®¶

---

å…ˆæ„Ÿè°¢ç€å‡ ä½å¤§ä½¬ğŸ­
[æ–°å¹´å‰çˆ¬ä¸ªæ˜æ—¥æ–¹èˆŸçš„ç«‹ç»˜](https://noionion.top/53760.html)
[æ˜æ—¥æ–¹èˆŸå¹²å‘˜ç«‹ç»˜çˆ¬è™«](https://www.heart-of-engine.top/posts/fccf.html)
[CloudFlare Workeræ­å»ºéšæœºå›¾ç‰‡API](https://blog.sukiu.top/Mixed/Random-Image-Worker/)
æ— æ„é—´åˆ·åˆ°äº†[å¤§ä½¬](https://noionion.top/)çš„åšå®¢ï¼Œçœ‹åˆ°äº†è¿™ä¸ªæ˜æ—¥æ–¹èˆŸçš„ç«‹ç»˜æŒºæœ‰æ„æ€çš„ï¼Œæƒ³ç›´æ¥æŠ„ä½œä¸šï¼Œä½†å¥ˆä½•ä»£ç å¤±æ•ˆäº†ï¼Œå°±è‡ªå·±æ”¹äº†ä¸€ä¸‹ï¼Œå‘å‡ºæ¥è®°å½•ä¸€ä¸‹

---

çœ‹äº†ä¸€ä¸‹ï¼Œåº”è¯¥æ˜¯åé¢çˆ¬é“¾æ¥çš„é‚£ä¸€å—å¤±æ•ˆäº†ï¼ŒåŸæ–‡æ˜¯è¿™æ ·çš„"**ç»™çš„æ˜¯ /images/thumb/6/65/%E7%AB%8B%E7%BB%98_%E5%87%AF%E5%B0%94%E5%B8%8C_2.png ï¼Œæˆ‘è¦çš„æ˜¯è¿™ä¸ª http://prts.wiki/images/6/65/%E7%AB%8B%E7%BB%98_%E5%87%AF%E5%B0%94%E5%B8%8C_2.png å˜›ï¼**â€
ç°åœ¨åº”è¯¥æ˜¯æ›´æ–°äº†ï¼Œç»™çš„é“¾æ¥æ˜¯**https://prts.wiki/w/%E6%96%87%E4%BB%B6:%E7%AB%8B%E7%BB%98_12F_1.png**ï¼Œä½†éœ€è¦çš„æ˜¯**https://media.prts.wiki/6/61/%E7%AB%8B%E7%BB%98_12F_1.png**

ç°åœ¨å®Œå…¨ç‰›å¤´ä¸å¯¹é©¬å˜´å˜›ğŸŒš

---

æ‰€ä»¥ç ”ç©¶äº†ä¸€ä¸‹ä¹‹åï¼Œæ„Ÿè§‰åº”è¯¥å¯ä»¥
æˆ‘å°±ç›´æ¥ä¸Šä»£ç äº†

```Python
# å¼•å…¥ç¨‹åºéœ€è¦çš„ç›¸åº”åŒ…ï¼ˆç±»ä¼¼Cè¯­è¨€ä¸­çš„# include<stdio.h> ã€include<math.h>ç­‰ï¼‰
import os
from bs4 import BeautifulSoup
import time
import requests

# è®¾ç½®çˆ¬å–å›¾ç‰‡çš„ç½‘å€
url = "http://prts.wiki/index.php?title=%E7%89%B9%E6%AE%8A:%E6%90%9C%E7%B4%A2&limit=500&offset=0&profile=images&search=%E7%AB%8B%E7%BB%98"


# é‡å®šä¹‰è¯·æ±‚å¤´ï¼Œé˜²æ­¢è¢«ç½‘é¡µå‘ç°æ˜¯çˆ¬è™«ï¼Œä»è€Œè¿›è¡Œåçˆ¬æ“ä½œ
headers = {
    "Cookie": "arccount62298=c; arccount62019=c",
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36 Edg/87.0.664.66"
}


# çˆ¬å–æ¨¡å—
html = requests.get(url, headers=headers) ##  è¿™é‡Œæ˜¯ä½¿ç”¨requestså‡½æ•°ç”¨ä¹‹å‰é‡å®šä¹‰çš„è¯·æ±‚å¤´è¯»å–ç½‘é¡µä¿¡æ¯
### æµ‹è¯•ä»£ç 
### print(html.text)
html.encoding = html.apparent_encoding ## encodingæ˜¯ä»httpä¸­çš„headerä¸­çš„charsetå­—æ®µä¸­æå–çš„ç¼–ç æ–¹å¼ï¼Œè‹¥headerä¸­æ²¡æœ‰charsetå­—æ®µåˆ™é»˜è®¤ä¸ºISO-8859-1ç¼–ç æ¨¡å¼ï¼Œæ— æ³•è§£æä¸­æ–‡ï¼Œè¿™æ˜¯ä¹±ç çš„åŸå› ,apparent_encodingä¼šä»ç½‘é¡µçš„å†…å®¹ä¸­åˆ†æç½‘é¡µç¼–ç çš„æ–¹å¼ï¼Œæ‰€ä»¥apparent_encodingæ¯”encodingæ›´åŠ å‡†ç¡®ï¼Œå½“ç½‘é¡µå‡ºç°ä¹±ç æ—¶å¯ä»¥æŠŠapparent_encodingçš„ç¼–ç æ ¼å¼èµ‹å€¼ç»™encodingã€‚
soup = BeautifulSoup(html.text, "html.parser") ## "html.parser"æ˜¯ä¸ºäº†è®©è§£æé€Ÿåº¦å¿«ï¼Œå®¹é”™ç‡é«˜ï¼Œå…·ä½“è®²è§£ç½‘å€ä¸ºhttps://blog.csdn.net/yangjiajia123456/article/details/80959896
list = soup.find_all(class_ = "searchResultImage") ## æ³¨ä¸€ï¼šå¯¹çˆ¬å–é¡µé¢è¿›è¡Œâ€œæ£€æŸ¥â€æ“ä½œåå‘ç°ï¼Œæ‰€æœ‰çš„å›¾ç‰‡éƒ½åŒ…å«åœ¨ä¸€ä¸ª<table class="searchResultImage">çš„classä¸­ï¼Œæ‰€ä»¥è¿™è¡Œä»£ç çš„ä½œç”¨æ˜¯æŸ¥æ‰¾htmlä»£ç ä¸­æ ‡ç­¾ä¸ºclassä¸”å€¼ä¸ºsearchRusultImggeçš„æ‰€æœ‰ä»£ç å—
### æµ‹è¯•ä»£ç 
### print(list)
### å¯ä»¥çœ‹åˆ°ï¼Œç›¸æ¯”äºhtml.textï¼Œlisté‡Œæ²¡æœ‰å¤§é‡çš„ç½‘é¡µé…ç½®ä¿¡æ¯ï¼Œåªæœ‰ä»¥å›¾ç‰‡åç§°ä¸ºæ ¸å¿ƒçš„åˆ—è¡¨


# æ£€æŸ¥é¡¹ç›®æ–‡ä»¶å¤¹ä¸‹æ˜¯å¦æœ‰â€œArknightsâ€è¿™ä¸ªæ–‡ä»¶å¤¹ï¼Œæ²¡æœ‰çš„è¯æ–°å»ºä¸€ä¸ªï¼Œæœ‰çš„è¯å°±ä¸æ–°å»ºäº†
try:
    os.mkdir("./Arknights")  ##  æ³¨äºŒï¼šåˆ›å»ºæ–‡ä»¶å¤¹ï¼ˆè¿™ä¸ªæ–‡ä»¶å¤¹åˆ›å»ºåœ¨ç°åœ¨çš„é¡¹ç›®æ–‡ä»¶å¤¹é‡Œï¼Œæ¯”æ–¹è¯´æˆ‘è¿™é‡Œç”¨çš„æ˜¯â€œæ˜æ—¥æ–¹èˆŸå¹²å‘˜ç«‹ç»˜é¡¹ç›®â€é¡¹ç›®æ–‡ä»¶å¤¹ï¼Œçˆ¬å–åˆ°çš„å›¾ç‰‡å°±å­˜åœ¨â€œæ˜æ—¥æ–¹èˆŸå¹²å‘˜ç«‹ç»˜é¡¹ç›®â€é¡¹ç›®æ–‡ä»¶å¤¹ä¸‹çš„â€œArknightsâ€æ–‡ä»¶å¤¹é‡Œï¼Œè¯¥å‡½æ•°å…·ä½“ä»‹ç»ç½‘å€ä¸ºhttps://blog.csdn.net/qq_20412595/article/details/82423764ï¼‰
    ## é¡¹ç›®æ–‡ä»¶å¤¹é»˜è®¤åœ¨ä»£ç å­˜æ”¾çš„åœ°æ–¹ï¼Œå¦‚æœä½ æ”¾åœ¨æ¡Œé¢ï¼Œé‚£åé¢ç”Ÿæˆçš„Arknightsæ–‡ä»¶å¤¹å°±åœ¨æ¡Œé¢
except:
    pass

# å°†å½“å‰å·¥ä½œè·¯å¾„åˆ‡æ¢ä¸ºâ€œArknightsâ€è¿™ä¸ªæ–‡ä»¶å¤¹
os.chdir("./Arknights") ##  åˆ‡æ¢å½“å‰å·¥ä½œè·¯å¾„ï¼ˆæœ¬å‡½æ•°çš„å…·ä½“ä»‹ç»åœ¨https://www.runoob.com/python3/python3-os-chdir.htmlï¼‰

# å°†æœ€åˆè¯»å–çš„å›¾ç‰‡è®¾ç½®ä¸ºç¬¬ä¸€å¼ å›¾ç‰‡
num=0

# æµ‹è¯•ç”¨å‚æ•°
# num1 = 0
# num2 = 0

# å›¾ç‰‡çš„çˆ¬å–ä¸æœ¬åœ°å­˜å‚¨
for s in list:
    string = str(s) ## å°†listä¸­çš„å•ä¸ªå…ƒç´ åŒ–ä¸ºå­—ç¬¦ä¸²ï¼Œå†é’ˆå¯¹è¯¥å­—ç¬¦ä¸²è¿›è¡Œå¤„ç†

    namebegin = string.find('title="æ–‡ä»¶') ##  æ³¨ä¸‰ï¼šæ˜¾ç„¶ï¼Œæ‰€æœ‰ç«‹ç»˜çš„æ–‡ä»¶åå¼€å¤´éƒ½æœ‰â€œæ–‡ä»¶â€äºŒå­—ï¼Œæ‰¾åˆ°â€œæ–‡ä»¶â€äºŒå­—å°±ç›¸å½“äºæ‰¾åˆ°äº†ç«‹ç»˜ æ³¨å››ï¼šä¸ºäº†è¯»å–å›¾ç‰‡é‡ŒåŒ…å«çš„å¹²å‘˜åï¼Œæˆ‘ä»¬å‘ç°èƒ½å¤Ÿé€šè¿‡titleå…¥æ‰‹ï¼Œå‘åæœç´¢åç§°ï¼Œè¿™é‡Œæ‰¾åˆ°äº†"title"ä¸­"tâ€çš„ä½ç½®
    # name_point_1 = namebegin+12 ## ä¸ºäº†è¯»å–å¹²å‘˜åç§°ï¼Œæˆ‘ä»¬è¿™é‡Œæ‰¾åˆ°ç¬¬ä¸€ä¸ªç©ºæ ¼çš„ä½ç½®name_point_1ï¼ˆè¿™é‡Œç©ºæ ¼åˆšå¥½åœ¨namebeginåçš„ç¬¬äºŒæ ¼ï¼‰
    # name_point_2 = string.find(' 1'or' 2'or' skin') ## è¿™é‡Œæ‰¾åˆ°ç¬¬äºŒä¸ªç©ºæ ¼çš„ä½ç½®name_point_2ï¼ˆæœ¬æ–¹æ³•è¿”å›å€¼è¿œå°äºnamebeginçš„æ•°å€¼ï¼Œæ˜¯é”™è¯¯çš„ï¼‰
    nameend = string[namebegin:].find('png') ## æ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶åæœ«å°¾çš„"png"å­—æ ·å¯¹åº”çš„ä½ç½®

    ## æµ‹è¯•ä»£ç 
    # print(namebegin)
    # print(name_point_1)
    # print(name_point_2)
    # print(nameend)

    ## è¿™é‡Œå¦‚æœä½¿ç”¨name_point_2 = string.find(' 1'or' 2'or' skin')ï¼Œä¼šæ˜¾ç¤ºä½ç½®ä¸º54ï¼Œè¿œå°äºnamebeginçš„ä½ç½®å€¼ï¼Œæ˜¯é”™è¯¯çš„ï¼Œæ‰€ä»¥ç”¨è¿™ç§æ–¹æ³•éš¾ä»¥æ‰¾åˆ°ç¬¬äºŒä¸ªç©ºæ ¼çš„ä½ç½®ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†"å¹²å‘˜å 1/2/skin(b/_V).png"æ•´ä½“ä»¤ä¸ºname1ï¼Œå†åœ¨name1é‡Œå¯»æ‰¾ç©ºæ ¼ä½ç½®
    name1 = string[namebegin+13:namebegin+nameend-1]
    name1begin = name1.find('"å¹²å‘˜å"')
    name1_point_1 = name1begin
    name1_point_2 = name1.find(' 1'or' 2'or' skin') ## æ ¹æ®æµ‹è¯•ä»£ç æˆ‘ä»¬æœ‰ï¼Œè¾“å‡ºç»“æœä¸º-1ï¼Œå³name1[name1begin]å¯¹åº”äºname1çš„æœ€åä¸€ä¸ªå­—ç¬¦

    # æµ‹è¯•ä»£ç 
    # print(name1) # è¾“å‡ºç»“æœæ ¼å¼"æš—ç´¢ skin1 V1"
    # print(name1begin)
    # print(name1[name1begin])
    # print(name1_point_1)
    # print(name1_point_2)

    # print(string[(namebegin+nameend-3):(namebegin+nameend-1)]) ## æœ¬ä»£ç ç”¨æ¥æŸ¥çœ‹åä¸¤ä½å¹¶è¿›è¡Œæ£€éªŒ

    if(string[namebegin+nameend-3:namebegin+nameend-1] in ['V1','V2']): ## è‹¥åä¸¤ä½ä¸ºV1æˆ–V2
        # num1+=1
        # print('num1={}'.format(num1))
        continue
    elif(string[namebegin+nameend-2:namebegin+nameend-1]=='b'): ## è‹¥æœ€åä¸€ä½ä¸ºb
        # num2+=1
        # print('num2={}'.format(num2))
        continue
    else:
        name1 = name1 + '.png'

    # æµ‹è¯•ä»£ç 
    # print(name1)

    # ä¿®æ”¹æ–‡ä»¶åç§°
    name1 = name1.replace(" ","_") ## å°†å›¾ç‰‡æ–‡ä»¶åé‡Œçš„ç©ºæ ¼è½¬ä¸ºâ€_â€œ

    # è®¾ç½®æ˜¾ç¤ºçš„é“¾æ¥å
    urlbegin = string.find('https://media.prts.wiki/thumb/')
    if urlbegin != -1:
        urlend = string.find('.png', urlbegin)
        imgurl_suffix = string[urlbegin+30:urlend+4]  # è·å–https://media.prts.wiki/thumb/åé¢çš„å†…å®¹
        imgurl = 'https://media.prts.wiki/' + imgurl_suffix
    # çˆ¬å–å›¾ç‰‡
    img = requests.get(imgurl, headers=headers).content

    # æµ‹è¯•ä»£ç 
    # print(name1)

    # å›¾ç‰‡å†™å…¥æœ¬åœ°æ–‡ä»¶å¤¹
    if(name1 not in ['ç²¾äºŒç«‹ç»˜A.png','Pith_2.png','Sharp_2.png','Stormeye_2.png','Touch_2.png','é˜¿ç±³å¨…(è¿‘å«)_2.png']): ## ä¸ºäº†ä¸è¯»å–â€ç²¾äºŒç«‹ç»˜Aâ€œä¸äº”ä½ç‰¹æ®Šå¹²å‘˜ï¼ˆå‰é¢åšæ–‡æœ‰è¯´åˆ°ï¼‰çš„å›¾ç‰‡ï¼Œä¸“é—¨åšäº†ä¸€ä¸ªå¾ªç¯ï¼ˆå…¶å®çˆ¬å–åå†ç›´æ¥å¯»æ‰¾åˆ é™¤ä¹Ÿå¯ä»¥ï¼‰
    ## ç»è¿‡noionionå¤§ä½¬çš„æç¤ºï¼Œorçš„ä¼˜å…ˆçº§ä¸é«˜ï¼Œæ‰€ä»¥ä¸èƒ½ç”¨if(name1 != 'ç²¾äºŒç«‹ç»˜A.png' or 'Pith_2.png' or 'Sharp_2.png' or 'Stormeye_2.png' or 'Touch_2.png' or 'é˜¿ç±³å¨…(è¿‘å«)_2.png'):å†™æ³•
    ## ä»–è¿˜å»ºè®®æˆ‘å¯¹åˆ—è¡¨è¿›è¡Œç¡¬æ“ä½œï¼Œè€Œä¸æ˜¯ç”¨!=è¿™ç§é€»è¾‘è¿ç®—ç¬¦ï¼ˆè¯´å¯èƒ½ä¼šç‚¸ï¼Œæˆ‘æ…Œçš„ä¸€æ‰¹ï¼‰
    ## æ‰€ä»¥æˆ‘å»é™¤äº†if(name1 != ('ç²¾äºŒç«‹ç»˜A.png' or 'Pith_2.png' or 'Sharp_2.png' or 'Stormeye_2.png' or 'Touch_2.png' or 'é˜¿ç±³å¨…(è¿‘å«)_2.png'))ï¼Œæ”¹ç”¨å…¨æ–°çš„åˆ¤æ–­è¯­æ³•
        with open(name1, 'wb') as f:
            f.write(img) ## å°†çˆ¬å–åˆ°çš„å›¾ç‰‡å†™å…¥â€œArknightsâ€è¿™ä¸ªæ–‡ä»¶å¤¹é‡Œ
            num+=1 ## num=num+1ï¼Œä½¿å¾—ä¸‹é¢printå‡½æ•°é‡Œçš„â€å·²çˆ¬å–{}å¼ â€œä¸­çš„{}å¯¹åº”æ•°å€¼åŠ 1
            print("å·²çˆ¬å–{}å¼ ,å›¾ç‰‡åç§°ä¸ºï¼š{}ï¼Œé“¾æ¥ä¸ºï¼š{}".format(num,name1,imgurl)) ## è¿™é‡Œ{}çš„æ„ä¹‰ä¸Cè¯­è¨€ä¸­çš„"%d"/"%f"ååˆ†ç›¸è¿‘ï¼Œæ˜¯ä»format()å‡½æ•°é‡Œè¯»å–å¯¹åº”ä½ç½®çš„å‚æ•°
        # "r"--ä»¥è¯»æ–¹å¼æ‰“å¼€,åªèƒ½è¯»æ–‡ä»¶,å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨,ä¼šå‘ç”Ÿå¼‚å¸¸
        # "w"--ä»¥å†™æ–¹å¼æ‰“å¼€,åªèƒ½å†™æ–‡ä»¶,å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨,åˆ›å»ºè¯¥æ–‡ä»¶;å¦‚æœæ–‡ä»¶å·²å­˜åœ¨,å…ˆæ¸…ç©º,å†æ‰“å¼€æ–‡ä»¶
        # "rb"--ä»¥äºŒè¿›åˆ¶è¯»æ–¹å¼æ‰“å¼€,åªèƒ½è¯»æ–‡ä»¶,å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨,ä¼šå‘ç”Ÿå¼‚å¸¸
        # "wb"--ä»¥äºŒè¿›åˆ¶å†™æ–¹å¼æ‰“å¼€,åªèƒ½å†™æ–‡ä»¶,å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨,åˆ›å»ºè¯¥æ–‡ä»¶;å¦‚æœæ–‡ä»¶å·²å­˜åœ¨,å…ˆæ¸…ç©º,å†æ‰“å¼€æ–‡ä»¶ï¼ˆæ‰€ä»¥è¿™ä¸ªç¨‹åºå°±ç®—è¿è¡Œå¤šæ¬¡ï¼Œæ–‡ä»¶å¤¹é‡Œä¹Ÿä¸ä¼šå‡ºç°é‡å¤çš„ç«‹ç»˜ï¼‰
        time.sleep(1) ## ä¼‘æ¯ä¸€ç§’åçˆ¬å–ä¸‹ä¸€å¼ å›¾ç‰‡ï¼Œè¿™ä¸ªå‡½æ•°åœ¨é“¾æ¥https://blog.csdn.net/weixin_45949073/article/details/104989562é‡Œè®²å¾—å¾ˆæ¸…æ¥š
```
2025åˆlookäº†ä¸€ä¸‹ï¼Œå‘ç°å¤±æ•ˆäº†ï¼Œåˆé‡æ–°å†™äº†ä¸€ä¸ª
```Python
import os
import requests
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor
from urllib.parse import unquote

# è®¾ç½®çˆ¬å–å›¾ç‰‡çš„ç½‘å€
url = "https://prts.wiki/index.php?title=%E7%89%B9%E6%AE%8A:%E6%90%9C%E7%B4%A2&limit=500&offset=0&profile=images&search=%E7%AB%8B%E7%BB%98"

# é‡å®šä¹‰è¯·æ±‚å¤´ï¼Œé˜²æ­¢è¢«ç½‘é¡µå‘ç°æ˜¯çˆ¬è™«
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36 Edg/87.0.664.66",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "zh-CN,zh;q=0.9",
    "Accept-Encoding": "gzip, deflate, br",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1"
}

def fetch_html(url, headers):
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        response.encoding = "utf-8"
        return response.text
    except requests.RequestException as e:
        print(f"è¯·æ±‚ç½‘é¡µæ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
        exit()

def parse_image_links(html):
    soup = BeautifulSoup(html, "html.parser")
    search_results = soup.find("div", class_="searchresults")
    image_links = []
    for result in search_results.find_all("li", class_="mw-search-result"):
        table = result.find("table", class_="searchResultImage")
        if table:
            td = table.find("td", style="vertical-align: top")
            if td:
                link = td.find("a")
                if link and link.get("href"):
                    image_page_url = "https://prts.wiki" + link.get("href")
                    image_links.append(image_page_url)
    return image_links

def download_image(image_page_url, headers):
    try:
        image_page_response = requests.get(image_page_url, headers=headers)
        image_page_response.raise_for_status()
        image_page_response.encoding = "utf-8"
        
        image_page_soup = BeautifulSoup(image_page_response.text, "html.parser")
        full_image_link = image_page_soup.find("div", class_="fullImageLink", id="file")
        if full_image_link:
            image_link = full_image_link.find("a")
            if image_link and image_link.get("href"):
                image_download_url = unquote(image_link.get("href"), encoding='utf-8').replace("+", "%2B")
                image_name = image_download_url.split("/")[-1]
                
                image_download_response = requests.get(image_download_url, headers=headers)
                image_download_response.raise_for_status()
                with open(image_name, "wb") as f:
                    f.write(image_download_response.content)
                print(f"å·²æˆåŠŸä¸‹è½½å›¾ç‰‡ï¼š{image_name}")
    except requests.RequestException as e:
        print(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥ï¼š{image_page_url}ï¼Œé”™è¯¯åŸå› ï¼š{e}")

def main():
    html = fetch_html(url, headers)
    image_links = parse_image_links(html)
    
    if not os.path.exists("./Arknights"):
        os.mkdir("./Arknights")
    os.chdir("./Arknights")
    
    with ThreadPoolExecutor(max_workers=10) as executor:
        executor.map(lambda link: download_image(link, headers), image_links)

if __name__ == "__main__":
    main()
```




~~è¿™é‡Œå‚ç…§äº†[https://www.heart-of-engine.top/posts/fccf.html](https://www.heart-of-engine.top/posts/fccf.html)çš„ä»£ç ï¼Œå°å°æ”¹åŠ¨äº†ä¸€ç‚¹ï¼ŒæˆåŠŸè¿è¡Œèµ·æ¥äº†ğŸŒ~~2025çš„å°±æ˜¯å®Œå…¨è‡ªå·±å†™çš„äº†ğŸ˜…

---

åé¢ç”±äºæƒ³åšæˆéšæœºå›¾ç‰‡apiæ‰€ä»¥å°±ç”¨CloudFlare Workeræ­å»ºäº†
ç›´æ¥ä¸Šæ•™ç¨‹äº†

---

å›¾ç‰‡è½¬ Webp
è¿™æ­¥éª¤å¯é€‰ï¼Œè½¬æ¢ä¸º webp æ ¼å¼åï¼Œå›¾ç‰‡ä½“ç§¯å‹ç¼©è€Œåˆ†è¾¨ç‡ä¸å˜ï¼Œå¯ä»¥æ›´å¿«åœ°åŠ è½½ã€‚

ä¸ºäº†åœ¨å¼•ç”¨æ—¶æ–¹ä¾¿ï¼Œå…ˆæŠŠåç§°æŒ‰ 1ã€2ã€3â€¦é¡ºåºæ ‡å·ï¼Œå†é€šè¿‡ PIL åº“ï¼Œå°† jpg å’Œ png æ ¼å¼çš„å›¾ç‰‡è½¬æ¢æˆ webpã€‚

```Python
import os
from PIL import Image

def rename_and_convert_images(directory):
    # è·å–ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
    files = os.listdir(directory)
    # è¿‡æ»¤å‡º png å’Œ webp æ ¼å¼çš„æ–‡ä»¶
    png_files = [f for f in files if f.lower().endswith('.png')]
    webp_files = [f for f in files if f.lower().endswith('.webp')]
  
    # æ‰¾åˆ°ç°æœ‰çš„æœ€å¤§åºå·
    max_index = 0
    for f in webp_files:
        try:
            index = int(os.path.splitext(f)[0])
            if index > max_index:
                max_index = index
        except ValueError:
            continue
  
    # æŒ‰é¡ºåºé‡å‘½åæ–‡ä»¶å¹¶è½¬æ¢æ ¼å¼
    for index, filename in enumerate(png_files, start=max_index + 1):
        # æ„å»ºæ–°çš„æ–‡ä»¶å
        new_filename = f"{index}.webp"
        # æ„å»ºå®Œæ•´çš„æ–‡ä»¶è·¯å¾„
        old_filepath = os.path.join(directory, filename)
        new_filepath = os.path.join(directory, new_filename)
      
        # æ‰“å¼€å›¾åƒå¹¶è½¬æ¢ä¸º webp æ ¼å¼
        with Image.open(old_filepath) as img:
            img.save(new_filepath, 'webp')
      
        # åˆ é™¤åŸå§‹æ–‡ä»¶
        os.remove(old_filepath)
        print(f"Converted {filename} to {new_filename}")

# ä½¿ç”¨ç¤ºä¾‹
directory = './Arknights'  # æ›¿æ¢ä¸ºä½ çš„ç›®å½•è·¯å¾„
rename_and_convert_images(directory)
```

---

æ–‡ä»¶ä¸Šä¼  Github
è¿™æ­¥éª¤ä¸å…·ä½“è¯´äº†ï¼Œå›¾ç‰‡é€šè¿‡ *jsdeliver* å¼•ç”¨ã€‚

---

CloudFlare Worker
æ–°å»ºä¸€ä¸ª workerï¼Œä»£ç å‚è€ƒå¦‚ä¸‹ï¼š

+å› ä¸ºæ”¹äº†æ–‡ä»¶åï¼Œæ‰€ä»¥ä¸‹é¢åªéœ€è¦ä¿®æ”¹å›¾ç‰‡æ€»æ•° totalï¼Œå°±èƒ½ç”Ÿæˆä¸€ä¸ªéšæœºæ•°è®¿é—®
+ä¸ªäººé€šè¿‡ type åŒºåˆ«å®½å±å’Œç«–å±å›¾ç‰‡ï¼Œè‡ªå·±æŒ‰éœ€ä¿®æ”¹

```JS
addEventListener('fetch', event => {
    event.respondWith(
        handleRequest(event.request).catch((err) =>
            new Response('cfworker error:\n' + err.stack, {
                status: 502,
            })
        )
    );
});

async function handleRequest(request) {
    const url = new URL(request.url);
    switch (url.pathname) {
        case "/img":
            var type = url.searchParams.has("type") ? url.searchParams.get("type") : "pc";
            const total = getTotal(type);
            if (total == 0) return handleImage("pc", getTotal("pc"));
            return handleImage(type, total);
        case "/favicon.ico":
            return fetch("https://gcore.jsdelivr.net/gh/SukiEva/assets/blog/favicon.ico");
        default:
            return handleNotFound();
    }
}

function getTotal(type) {
    switch (type) {
        case "pc": return 175;
        case "mb": return 0;
        default: return 0;
    }
}

async function handleImage(type, total) {
    var index = Math.floor((Math.random() * total)) + 1;
    var img = "https://gcore.jsdelivr.net/gh/SukiEva/assets/webp/" + type + "/" + index + ".webp";
    res = await fetch(img);
    return new Response(res.body, {
        headers: {
            'content-type': 'image/webp',
        },
    });
}

function handleNotFound() {
    return new Response('Not Found', {
        status: 404,
    });
}
```

---

è‡ªå®šä¹‰åŸŸå
é»˜è®¤æ˜¯ä¸€ä¸ª CF çš„åŸŸåï¼Œä½†æ˜¯ç›®å‰å·²ç»è¢«å›½å†… ban äº†ï¼Œéœ€è¦è‡ªè¡Œæ·»åŠ åŸŸåè·¯ç”±ã€‚

æœ€åå†æ¬¡ç¥çœ‹åˆ°è¿™ç¯‡åšå®¢çš„å°ä¼™ä¼´ï¼š

## 2024ï¼Œå›½åº†å¿«ä¹é¸­!

åœ¨å°å°çš„æ´é‡Œçˆ¬å‘€çˆ¬å‘€çˆ¬ï¼ˆ é€ƒ
